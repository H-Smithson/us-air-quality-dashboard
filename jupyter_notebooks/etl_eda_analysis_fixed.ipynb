{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42578cff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:50:22.048918Z",
     "iopub.status.busy": "2025-11-13T09:50:22.033731Z",
     "iopub.status.idle": "2025-11-13T09:50:22.070077Z",
     "shell.execute_reply": "2025-11-13T09:50:22.070077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: c:\\Projects\\us-air-quality-dashboard\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "cwd = Path.cwd()\n",
    "if cwd.name == 'jupyter_notebooks':\n",
    "    os.chdir(cwd.parent)\n",
    "print('Working dir:', Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18207496",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:50:22.070077Z",
     "iopub.status.busy": "2025-11-13T09:50:22.070077Z",
     "iopub.status.idle": "2025-11-13T09:50:24.909544Z",
     "shell.execute_reply": "2025-11-13T09:50:24.909544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 rows from Data\\pollution_us_2000_2016.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>State Code</th>\n",
       "      <th>County Code</th>\n",
       "      <th>Site Num</th>\n",
       "      <th>Address</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>City</th>\n",
       "      <th>Date Local</th>\n",
       "      <th>NO2 Units</th>\n",
       "      <th>...</th>\n",
       "      <th>SO2 Units</th>\n",
       "      <th>SO2 Mean</th>\n",
       "      <th>SO2 1st Max Value</th>\n",
       "      <th>SO2 1st Max Hour</th>\n",
       "      <th>SO2 AQI</th>\n",
       "      <th>CO Units</th>\n",
       "      <th>CO Mean</th>\n",
       "      <th>CO 1st Max Value</th>\n",
       "      <th>CO 1st Max Hour</th>\n",
       "      <th>CO AQI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>1.145833</td>\n",
       "      <td>4.2</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>2.2</td>\n",
       "      <td>23</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>2.975000</td>\n",
       "      <td>6.6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>1.145833</td>\n",
       "      <td>4.2</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>2.975000</td>\n",
       "      <td>6.6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>2.2</td>\n",
       "      <td>23</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>1.958333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>1.6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  State Code  County Code  Site Num  \\\n",
       "0           0           4           13      3002   \n",
       "1           1           4           13      3002   \n",
       "2           2           4           13      3002   \n",
       "3           3           4           13      3002   \n",
       "4           4           4           13      3002   \n",
       "\n",
       "                                   Address    State    County     City  \\\n",
       "0  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN  Arizona  Maricopa  Phoenix   \n",
       "1  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN  Arizona  Maricopa  Phoenix   \n",
       "2  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN  Arizona  Maricopa  Phoenix   \n",
       "3  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN  Arizona  Maricopa  Phoenix   \n",
       "4  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN  Arizona  Maricopa  Phoenix   \n",
       "\n",
       "   Date Local          NO2 Units  ...          SO2 Units  SO2 Mean  \\\n",
       "0  2000-01-01  Parts per billion  ...  Parts per billion  3.000000   \n",
       "1  2000-01-01  Parts per billion  ...  Parts per billion  3.000000   \n",
       "2  2000-01-01  Parts per billion  ...  Parts per billion  2.975000   \n",
       "3  2000-01-01  Parts per billion  ...  Parts per billion  2.975000   \n",
       "4  2000-01-02  Parts per billion  ...  Parts per billion  1.958333   \n",
       "\n",
       "   SO2 1st Max Value  SO2 1st Max Hour SO2 AQI           CO Units   CO Mean  \\\n",
       "0                9.0                21    13.0  Parts per million  1.145833   \n",
       "1                9.0                21    13.0  Parts per million  0.878947   \n",
       "2                6.6                23     NaN  Parts per million  1.145833   \n",
       "3                6.6                23     NaN  Parts per million  0.878947   \n",
       "4                3.0                22     4.0  Parts per million  0.850000   \n",
       "\n",
       "   CO 1st Max Value  CO 1st Max Hour CO AQI  \n",
       "0               4.2               21    NaN  \n",
       "1               2.2               23   25.0  \n",
       "2               4.2               21    NaN  \n",
       "3               2.2               23   25.0  \n",
       "4               1.6               23    NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "src = Path('Data/pollution_us_2000_2016.csv')\n",
    "fallback = Path('outputs/pollution_us_200_2016_clean.csv')\n",
    "if src.exists():\n",
    "    df = pd.read_csv(src, nrows=10000)\n",
    "    print(f'Loaded {len(df)} rows from {src}')\n",
    "elif fallback.exists():\n",
    "    df = pd.read_csv(fallback, nrows=10000)\n",
    "    print(f'Fallback: loaded {len(df)} rows from {fallback}')\n",
    "else:\n",
    "    print('No data available; creating empty DataFrame')\n",
    "    df = pd.DataFrame()\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e86c9fde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:50:24.909544Z",
     "iopub.status.busy": "2025-11-13T09:50:24.909544Z",
     "iopub.status.idle": "2025-11-13T09:51:32.682595Z",
     "shell.execute_reply": "2025-11-13T09:51:32.682595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote cleaned CSV to outputs\\pollution_us_200_2016_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# Chunk-processing (only runs if raw exists)\n",
    "from pathlib import Path\n",
    "file_path = Path('Data/pollution_us_2000_2016.csv')\n",
    "out_path = Path('outputs/pollution_us_200_2016_clean.csv')\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "if file_path.exists():\n",
    "    if out_path.exists():\n",
    "        out_path.unlink()\n",
    "    chunks = pd.read_csv(file_path, chunksize=200_000)\n",
    "    first = True\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk['Date Local'] = pd.to_datetime(chunk.get('Date Local'), errors='coerce')\n",
    "        chunk['Year'] = chunk['Date Local'].dt.year\n",
    "        chunk['Month'] = chunk['Date Local'].dt.month\n",
    "        chunk['Quarter'] = chunk['Date Local'].dt.quarter\n",
    "        chunk.to_csv(out_path, mode='a', index=False, header=first)\n",
    "        first = False\n",
    "    print('Wrote cleaned CSV to', out_path)\n",
    "else:\n",
    "    print('Raw file missing; chunk-processing skipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe884417",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:51:32.686997Z",
     "iopub.status.busy": "2025-11-13T09:51:32.686997Z",
     "iopub.status.idle": "2025-11-13T09:51:32.787224Z",
     "shell.execute_reply": "2025-11-13T09:51:32.786617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview path: outputs\\pollution_us_200_2016_clean.csv\n",
      "Sampled cleaned CSV rows: 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unnamed:_0</th>\n",
       "      <th>state_code</th>\n",
       "      <th>county_code</th>\n",
       "      <th>site_num</th>\n",
       "      <th>address</th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "      <th>city</th>\n",
       "      <th>date_local</th>\n",
       "      <th>no2_units</th>\n",
       "      <th>...</th>\n",
       "      <th>so2_1st_max_hour</th>\n",
       "      <th>so2_aqi</th>\n",
       "      <th>co_units</th>\n",
       "      <th>co_mean</th>\n",
       "      <th>co_1st_max_value</th>\n",
       "      <th>co_1st_max_hour</th>\n",
       "      <th>co_aqi</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>1.145833</td>\n",
       "      <td>4.2</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>2.2</td>\n",
       "      <td>23</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>1.145833</td>\n",
       "      <td>4.2</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>2.2</td>\n",
       "      <td>23</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>1.6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unnamed:_0  state_code  county_code  site_num  \\\n",
       "0           0           4           13      3002   \n",
       "1           1           4           13      3002   \n",
       "2           2           4           13      3002   \n",
       "3           3           4           13      3002   \n",
       "4           4           4           13      3002   \n",
       "\n",
       "                                   address    state    county     city  \\\n",
       "0  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN  Arizona  Maricopa  Phoenix   \n",
       "1  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN  Arizona  Maricopa  Phoenix   \n",
       "2  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN  Arizona  Maricopa  Phoenix   \n",
       "3  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN  Arizona  Maricopa  Phoenix   \n",
       "4  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN  Arizona  Maricopa  Phoenix   \n",
       "\n",
       "   date_local          no2_units  ...  so2_1st_max_hour  so2_aqi  \\\n",
       "0  2000-01-01  Parts per billion  ...                21     13.0   \n",
       "1  2000-01-01  Parts per billion  ...                21     13.0   \n",
       "2  2000-01-01  Parts per billion  ...                23      NaN   \n",
       "3  2000-01-01  Parts per billion  ...                23      NaN   \n",
       "4  2000-01-02  Parts per billion  ...                22      4.0   \n",
       "\n",
       "            co_units   co_mean co_1st_max_value  co_1st_max_hour  co_aqi  \\\n",
       "0  Parts per million  1.145833              4.2               21     NaN   \n",
       "1  Parts per million  0.878947              2.2               23    25.0   \n",
       "2  Parts per million  1.145833              4.2               21     NaN   \n",
       "3  Parts per million  0.878947              2.2               23    25.0   \n",
       "4  Parts per million  0.850000              1.6               23     NaN   \n",
       "\n",
       "   year  month quarter  \n",
       "0  2000      1       1  \n",
       "1  2000      1       1  \n",
       "2  2000      1       1  \n",
       "3  2000      1       1  \n",
       "4  2000      1       1  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preview outputs (supports multiple historical filenames)\n",
    "from pathlib import Path\n",
    "candidates = [Path('outputs/pollution_us_200_2016_clean.csv'), Path('outputs/pollution_us_2000_2016_clean.csv'), Path('outputs/pollution_us_200_2016_clean.csv')]\n",
    "preview = next((p for p in candidates if p.exists()), None)\n",
    "print('Preview path:', preview)\n",
    "if preview is not None:\n",
    "    # read a small sample to avoid memory pressure\n",
    "    df_clean = pd.read_csv(preview, low_memory=False, nrows=20)\n",
    "    print('Sampled cleaned CSV rows:', len(df_clean))\n",
    "    df_clean.columns = (df_clean.columns.str.strip().str.lower().str.replace(r'\\s+', '_', regex=True).str.replace('-', '').str.replace('/', '_'))\n",
    "    display(df_clean.head())\n",
    "else:\n",
    "    print('No cleaned CSV present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5283f4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:51:32.787224Z",
     "iopub.status.busy": "2025-11-13T09:51:32.787224Z",
     "iopub.status.idle": "2025-11-13T09:53:22.953942Z",
     "shell.execute_reply": "2025-11-13T09:53:22.949330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning numeric columns in: outputs\\pollution_us_200_2016_clean.csv\n",
      "Numeric columns: ['Unnamed: 0', 'State Code', 'County Code', 'Site Num', 'NO2 Mean', 'NO2 1st Max Value', 'NO2 1st Max Hour', 'NO2 AQI', 'O3 Mean', 'O3 1st Max Value', 'O3 1st Max Hour', 'O3 AQI', 'SO2 Mean', 'SO2 1st Max Value', 'SO2 1st Max Hour', 'SO2 AQI', 'CO Mean', 'CO 1st Max Value', 'CO 1st Max Hour', 'CO AQI', 'Year', 'Month', 'Quarter']\n",
      "Using medians for fill (sample):\n",
      "  Unnamed: 0: 999.5\n",
      "  State Code: 4.0\n",
      "  County Code: 13.0\n",
      "  Site Num: 3002.0\n",
      "  NO2 Mean: 29.6742425\n",
      "  NO2 1st Max Value: 55.0\n",
      "  NO2 1st Max Hour: 19.0\n",
      "  NO2 AQI: 52.0\n",
      "  O3 Mean: 0.021125\n",
      "  O3 1st Max Value: 0.043\n",
      "  O3 1st Max Hour: 10.0\n",
      "  O3 AQI: 36.0\n",
      "  SO2 Mean: 1.4666665\n",
      "  SO2 1st Max Value: 4.0\n",
      "  SO2 1st Max Hour: 8.0\n",
      "  SO2 AQI: 7.0\n",
      "  CO Mean: 0.7625\n",
      "  CO 1st Max Value: 1.7\n",
      "  CO 1st Max Hour: 7.0\n",
      "  CO AQI: 16.0\n",
      "  Year: 2000.0\n",
      "  Month: 6.0\n",
      "  Quarter: 2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numeric-clean summary:\n",
      "  total rows scanned: 1746661\n",
      "\n",
      "Top columns by percent-null (before -> after):\n",
      "  CO AQI: 50.00% -> 0.00% (nulls 873323 -> 0)\n",
      "  SO2 AQI: 49.98% -> 0.00% (nulls 872907 -> 0)\n",
      "  Unnamed: 0: 0.00% -> 0.00% (nulls 0 -> 0)\n",
      "  State Code: 0.00% -> 0.00% (nulls 0 -> 0)\n",
      "  County Code: 0.00% -> 0.00% (nulls 0 -> 0)\n",
      "  Site Num: 0.00% -> 0.00% (nulls 0 -> 0)\n",
      "  Address: 0.00% -> 0.00% (nulls 0 -> 0)\n",
      "  State: 0.00% -> 0.00% (nulls 0 -> 0)\n",
      "\n",
      "Sample after numeric coercion/fill (first 5 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>State Code</th>\n",
       "      <th>County Code</th>\n",
       "      <th>Site Num</th>\n",
       "      <th>Address</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>City</th>\n",
       "      <th>Date Local</th>\n",
       "      <th>NO2 Units</th>\n",
       "      <th>...</th>\n",
       "      <th>SO2 1st Max Hour</th>\n",
       "      <th>SO2 AQI</th>\n",
       "      <th>CO Units</th>\n",
       "      <th>CO Mean</th>\n",
       "      <th>CO 1st Max Value</th>\n",
       "      <th>CO 1st Max Hour</th>\n",
       "      <th>CO AQI</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>1.145833</td>\n",
       "      <td>4.2</td>\n",
       "      <td>21</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>2.2</td>\n",
       "      <td>23</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>1.145833</td>\n",
       "      <td>4.2</td>\n",
       "      <td>21</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>2.2</td>\n",
       "      <td>23</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>1.6</td>\n",
       "      <td>23</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  State Code  County Code  Site Num  \\\n",
       "0           0           4           13      3002   \n",
       "1           1           4           13      3002   \n",
       "2           2           4           13      3002   \n",
       "3           3           4           13      3002   \n",
       "4           4           4           13      3002   \n",
       "\n",
       "                                   Address    State    County     City  \\\n",
       "0  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN  Arizona  Maricopa  Phoenix   \n",
       "1  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN  Arizona  Maricopa  Phoenix   \n",
       "2  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN  Arizona  Maricopa  Phoenix   \n",
       "3  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN  Arizona  Maricopa  Phoenix   \n",
       "4  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN  Arizona  Maricopa  Phoenix   \n",
       "\n",
       "   Date Local          NO2 Units  ...  SO2 1st Max Hour  SO2 AQI  \\\n",
       "0  2000-01-01  Parts per billion  ...                21     13.0   \n",
       "1  2000-01-01  Parts per billion  ...                21     13.0   \n",
       "2  2000-01-01  Parts per billion  ...                23      7.0   \n",
       "3  2000-01-01  Parts per billion  ...                23      7.0   \n",
       "4  2000-01-02  Parts per billion  ...                22      4.0   \n",
       "\n",
       "            CO Units   CO Mean CO 1st Max Value  CO 1st Max Hour  CO AQI  \\\n",
       "0  Parts per million  1.145833              4.2               21    16.0   \n",
       "1  Parts per million  0.878947              2.2               23    25.0   \n",
       "2  Parts per million  1.145833              4.2               21    16.0   \n",
       "3  Parts per million  0.878947              2.2               23    25.0   \n",
       "4  Parts per million  0.850000              1.6               23    16.0   \n",
       "\n",
       "   Year  Month Quarter  \n",
       "0  2000      1       1  \n",
       "1  2000      1       1  \n",
       "2  2000      1       1  \n",
       "3  2000      1       1  \n",
       "4  2000      1       1  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote numeric-fixed CSV to: outputs\\pollution_us_200_2016_clean_numeric_fixed.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Ensure numeric columns are detected and cleaned, then report summary and save numeric-fixed CSV\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# locate the cleaned CSV (supports historical names)\n",
    "candidates = [Path('outputs/pollution_us_200_2016_clean.csv'), Path('outputs/pollution_us_2000_2016_clean.csv')]\n",
    "clean_path = next((p for p in candidates if p.exists()), None)\n",
    "if clean_path is None:\n",
    "    print('No cleaned CSV found; skipping numeric-clean step')\n",
    "else:\n",
    "    print('Cleaning numeric columns in:', clean_path)\n",
    "    # read a modest sample to infer dtypes and compute medians for filling\n",
    "    sample = pd.read_csv(clean_path, nrows=2000)\n",
    "\n",
    "    # Try to detect numeric columns from the sample first\n",
    "    numeric_cols = [c for c in sample.columns if pd.api.types.is_numeric_dtype(sample[c])]\n",
    "\n",
    "    # If none detected, attempt coercion heuristic: column is numeric if >=50% of non-null sample values coerce to numeric\n",
    "    if not numeric_cols:\n",
    "        numeric_cols = []\n",
    "        for c in sample.columns:\n",
    "            non_null = sample[c].notna().sum()\n",
    "            if non_null == 0:\n",
    "                continue\n",
    "            coerced = pd.to_numeric(sample[c], errors='coerce')\n",
    "            frac = coerced.notna().sum() / float(non_null)\n",
    "            if frac >= 0.5:\n",
    "                numeric_cols.append(c)\n",
    "\n",
    "    print('Numeric columns:', numeric_cols)\n",
    "\n",
    "    # compute medians from the sample for filling later (only for the detected numeric cols)\n",
    "    medians = {}\n",
    "    if numeric_cols:\n",
    "        medians = sample[numeric_cols].median().to_dict()\n",
    "        print('Using medians for fill (sample):')\n",
    "        for k, v in medians.items():\n",
    "            print(f'  {k}: {v}')\n",
    "\n",
    "    # prepare output path for numeric-fixed CSV (do not commit file by default)\n",
    "    out_numeric = Path('outputs/pollution_us_200_2016_clean_numeric_fixed.csv')\n",
    "    out_numeric.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_numeric.exists():\n",
    "        print('Removing existing numeric-fixed file:', out_numeric)\n",
    "        out_numeric.unlink()\n",
    "\n",
    "    # chunk through the full cleaned CSV, coerce numeric cols, fill missing with medians, write out, and compute null-change stats\n",
    "    chunksize = 200_000\n",
    "    total_rows = 0\n",
    "    null_before = {c: 0 for c in sample.columns}\n",
    "    null_after = {c: 0 for c in sample.columns}\n",
    "    first_write = True\n",
    "    for chunk in pd.read_csv(clean_path, chunksize=chunksize):\n",
    "        total_rows += len(chunk)\n",
    "        # accumulate before-null counts for all columns (coarse)\n",
    "        for c in sample.columns:\n",
    "            null_before[c] += int(chunk[c].isna().sum())\n",
    "        if numeric_cols:\n",
    "            for c in numeric_cols:\n",
    "                chunk[c] = pd.to_numeric(chunk[c], errors='coerce')\n",
    "                # fill with median from sample if available\n",
    "                if c in medians and pd.notna(medians[c]):\n",
    "                    chunk[c] = chunk[c].fillna(medians[c])\n",
    "        # accumulate after-null counts\n",
    "        for c in sample.columns:\n",
    "            null_after[c] += int(chunk[c].isna().sum())\n",
    "        # write chunk to numeric-fixed CSV\n",
    "        chunk.to_csv(out_numeric, mode='a', index=False, header=first_write)\n",
    "        first_write = False\n",
    "\n",
    "    # summarize\n",
    "    print('\\nNumeric-clean summary:')\n",
    "    print('  total rows scanned:', total_rows)\n",
    "    # report top columns by percent-null before\n",
    "    percent_before = {c: (null_before[c] / total_rows * 100) if total_rows>0 else 0 for c in sample.columns}\n",
    "    percent_after = {c: (null_after[c] / total_rows * 100) if total_rows>0 else 0 for c in sample.columns}\n",
    "    # show top 8 columns with highest null before\n",
    "    top_before = sorted(percent_before.items(), key=lambda x: x[1], reverse=True)[:8]\n",
    "    print('\\nTop columns by percent-null (before -> after):')\n",
    "    for c, p in top_before:\n",
    "        print(f'  {c}: {p:.2f}% -> {percent_after[c]:.2f}% (nulls {null_before[c]} -> {null_after[c]})')\n",
    "\n",
    "    # present small sample with numeric coercion applied for user inspection\n",
    "    sample2 = sample.copy()\n",
    "    if numeric_cols:\n",
    "        for c in numeric_cols:\n",
    "            sample2[c] = pd.to_numeric(sample2[c], errors='coerce')\n",
    "            if c in medians and pd.notna(medians[c]):\n",
    "                sample2[c] = sample2[c].fillna(medians[c])\n",
    "    print('\\nSample after numeric coercion/fill (first 5 rows):')\n",
    "    display(sample2.head())\n",
    "    print('\\nWrote numeric-fixed CSV to:', out_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d5fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Normalize headers to snake_case, standardize decimals, and impute AQI; write standardized CSV\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# input is the numeric-fixed CSV if present, otherwise fall back to cleaned CSVs\n",
    "in_path = Path('outputs/pollution_us_200_2016_clean_numeric_fixed.csv')\n",
    "fallbacks = [Path('outputs/pollution_us_200_2016_clean.csv'), Path('outputs/pollution_us_2000_2016_clean.csv')]\n",
    "if not in_path.exists():\n",
    "    in_path = next((p for p in fallbacks if p.exists()), None)\n",
    "if in_path is None:\n",
    "    print('No input file found for standardization; skipping Step 4')\n",
    "else:\n",
    "    out_path = Path('outputs/pollution_us_200_2016_clean_numeric_standardized.csv')\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_path.exists():\n",
    "        out_path.unlink()\n",
    "\n",
    "    def to_snake(s):\n",
    "        s = str(s).strip().lower()\n",
    "        s = re.sub(r'[\\\\/]', '_', s)\n",
    "        s = re.sub(r\"\\s+\", '_', s)\n",
    "        s = re.sub(r'[^0-9a-z_]', '', s)\n",
    "        s = re.sub(r'_+', '_', s)\n",
    "        return s\n",
    "\n",
    "    # read a sample to infer columns and candidate numeric columns\n",
    "    sample = pd.read_csv(in_path, nrows=2000)\n",
    "    col_map = {c: to_snake(c) for c in sample.columns}\n",
    "    sample = sample.rename(columns=col_map)\n",
    "\n",
    "    # detect numeric columns in sample (dtype based + coercion heuristic)\n",
    "    numeric_cols = [c for c in sample.columns if pd.api.types.is_numeric_dtype(sample[c])]\n",
    "    if not numeric_cols:\n",
    "        numeric_cols = []\n",
    "        for c in sample.columns:\n",
    "            non_null = sample[c].notna().sum()\n",
    "            if non_null == 0:\n",
    "                continue\n",
    "            coerced = pd.to_numeric(sample[c].astype(str).str.replace(',', '.'), errors='coerce')\n",
    "            if coerced.notna().sum() / float(non_null) >= 0.5:\n",
    "                numeric_cols.append(c)\n",
    "\n",
    "    print('Detected numeric columns:', numeric_cols)\n",
    "\n",
    "    # prepare AQI medians from sample if available\n",
    "    if 'aqi' in sample.columns and sample['aqi'].notna().any():\n",
    "        overall_aqi_med = sample['aqi'].dropna().median()\n",
    "        if 'year' in sample.columns:\n",
    "            aqi_by_year = sample.groupby('year')['aqi'].median().to_dict()\n",
    "        else:\n",
    "            aqi_by_year = {}\n",
    "    else:\n",
    "        overall_aqi_med = None\n",
    "        aqi_by_year = {}\n",
    "\n",
    "    chunksize = 200_000\n",
    "    total_rows = 0\n",
    "    aqi_before = 0\n",
    "    aqi_after = 0\n",
    "    first = True\n",
    "    for chunk in pd.read_csv(in_path, chunksize=chunksize):\n",
    "        # rename headers to snake_case consistently\n",
    "        chunk.rename(columns=col_map, inplace=True)\n",
    "        total_rows += len(chunk)\n",
    "\n",
    "        # standardize decimal separator on object columns (comma -> dot)\n",
    "        for col in chunk.select_dtypes(include=['object']).columns:\n",
    "            # only replace where commas appear to avoid unnecessary work\n",
    "            if chunk[col].astype(str).str.contains(',', na=False).any():\n",
    "                chunk[col] = chunk[col].astype(str).str.replace(',', '.', regex=False)\n",
    "\n",
    "        # coerce numeric columns we detected\n",
    "        for c in numeric_cols:\n",
    "            if c in chunk.columns:\n",
    "                chunk[c] = pd.to_numeric(chunk[c], errors='coerce')\n",
    "\n",
    "        # AQI imputation if present\n",
    "        if 'aqi' in chunk.columns:\n",
    "            aqi_before += int(chunk['aqi'].isna().sum())\n",
    "            if aqi_by_year:\n",
    "                # fill by year where possible, else overall median\n",
    "                def _fill(row):\n",
    "                    if pd.notna(row.get('aqi')):\n",
    "                        return row.get('aqi')\n",
    "                    y = row.get('year') if 'year' in row.index else None\n",
    "                    if pd.notna(y) and y in aqi_by_year and pd.notna(aqi_by_year[y]):\n",
    "                        return aqi_by_year[y]\n",
    "                    return overall_aqi_med\n",
    "                chunk['aqi'] = chunk.apply(_fill, axis=1)\n",
    "            else:\n",
    "                if overall_aqi_med is not None:\n",
    "                    chunk['aqi'] = chunk['aqi'].fillna(overall_aqi_med)\n",
    "            aqi_after += int(chunk['aqi'].isna().sum())\n",
    "\n",
    "        # write standardized chunk\n",
    "        chunk.to_csv(out_path, mode='a', index=False, header=first)\n",
    "        first = False\n",
    "\n",
    "    print('\\nWrote standardized CSV to:', out_path)\n",
    "    print('Total rows:', total_rows)\n",
    "    if overall_aqi_med is not None:\n",
    "        print('AQI missing before:', aqi_before, 'after:', aqi_after)\n",
    "    else:\n",
    "        print('AQI not present in data; no imputation performed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841a45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Calculate AQI per pollutant and compute overall AQI; write augmented CSV\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Input is the standardized CSV from Step 4 if present\n",
    "std_path = Path('outputs/pollution_us_200_2016_clean_numeric_standardized.csv')\n",
    "if not std_path.exists():\n",
    "    print('Standardized input not found; skipping AQI calculation (expected at', std_path, ')')\n",
    "else:\n",
    "    out_aqi = Path('outputs/pollution_us_200_2016_with_aqi.csv')\n",
    "    out_aqi.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_aqi.exists():\n",
    "        print('Removing existing AQI file:', out_aqi)\n",
    "        out_aqi.unlink()\n",
    "\n",
    "    # EPA-like breakpoints for AQI calculation (assumes concentrations in common units)\n",
    "    # Each entry is list of tuples: (C_low, C_high, I_low, I_high)\n",
    "    breakpoints = {\n",
    "        'pm25': [\n",
    "            (0.0, 12.0, 0, 50),\n",
    "            (12.1, 35.4, 51, 100),\n",
    "            (35.5, 55.4, 101, 150),\n",
    "            (55.5, 150.4, 151, 200),\n",
    "            (150.5, 250.4, 201, 300),\n",
    "            (250.5, 350.4, 301, 400),\n",
    "            (350.5, 500.4, 401, 500)\n",
    "        ],\n",
    "        'pm10': [\n",
    "            (0, 54, 0, 50),\n",
    "            (55, 154, 51, 100),\n",
    "            (155, 254, 101, 150),\n",
    "            (255, 354, 151, 200),\n",
    "            (355, 424, 201, 300),\n",
    "            (425, 504, 301, 400),\n",
    "            (505, 604, 401, 500)\n",
    "        ],\n",
    "        # SO2, NO2 are in ppb here; CO in ppm; O3 in ppb (8-hr)\n",
    "        'so2': [\n",
    "            (0, 35, 0, 50),\n",
    "            (36, 75, 51, 100),\n",
    "            (76, 185, 101, 150),\n",
    "            (186, 304, 151, 200),\n",
    "            (305, 604, 201, 300),\n",
    "            (605, 804, 301, 400),\n",
    "            (805, 1004, 401, 500)\n",
    "        ],\n",
    "        'no2': [\n",
    "            (0, 53, 0, 50),\n",
    "            (54, 100, 51, 100),\n",
    "            (101, 360, 101, 150),\n",
    "            (361, 649, 151, 200),\n",
    "            (650, 1249, 201, 300),\n",
    "            (1250, 1649, 301, 400),\n",
    "            (1650, 2049, 401, 500)\n",
    "        ],\n",
    "        'co': [\n",
    "            (0.0, 4.4, 0, 50),\n",
    "            (4.5, 9.4, 51, 100),\n",
    "            (9.5, 12.4, 101, 150),\n",
    "            (12.5, 15.4, 151, 200),\n",
    "            (15.5, 30.4, 201, 300),\n",
    "            (30.5, 40.4, 301, 400),\n",
    "            (40.5, 50.4, 401, 500)\n",
    "        ],\n",
    "        'o3': [\n",
    "            (0, 54, 0, 50),\n",
    "            (55, 70, 51, 100),\n",
    "            (71, 85, 101, 150),\n",
    "            (86, 105, 151, 200),\n",
    "            (106, 200, 201, 300)\n",
    "            # higher ranges omitted for brevity\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    def aqi_calc_from_breakpoints(C, bps):\n",
    "        \"\"\"Calculate AQI for a concentration C given breakpoint list bps\"\"\"\n",
    "        if C is None or (isinstance(C, float) and math.isnan(C)):\n",
    "            return None\n",
    "        try:\n",
    "            C = float(C)\n",
    "        except Exception:\n",
    "            return None\n",
    "        for (Cl, Ch, Il, Ih) in bps:\n",
    "            if Cl <= C <= Ch:\n",
    "                # linear interpolation\n",
    "                aqi = (Ih - Il) / (Ch - Cl) * (C - Cl) + Il\n",
    "                return int(round(aqi))\n",
    "        # if outside defined range, clip to 500 or return None\n",
    "        if C > bps[-1][1]:\n",
    "            return 500\n",
    "        return None\n",
    "\n",
    "    # find candidate column names in the standardized file that map to pollutants\n",
    "    sample = pd.read_csv(std_path, nrows=5)\n",
    "    cols = sample.columns.tolist()\n",
    "    pollutant_column_map = {}\n",
    "    # heuristics for column name detection\n",
    "    mapping_hints = {\n",
    "        'pm25': ['pm2_5', 'pm25', 'pm_2_5', 'pm2.5'],\n",
    "        'pm10': ['pm10', 'pm_10'],\n",
    "        'so2': ['so2', 's02', 'sulfur_dioxide'],\n",
    "        'no2': ['no2', 'nitrogen_dioxide'],\n",
    "        'co': ['co', 'carbon_monoxide'],\n",
    "        'o3': ['o3', 'ozone']\n",
    "    }\n",
    "    for pol, hints in mapping_hints.items():\n",
    "        for h in hints:\n",
    "            matches = [c for c in cols if h in c]\n",
    "            if matches:\n",
    "                pollutant_column_map[pol] = matches[0]\n",
    "                break\n",
    "\n",
    "    print('Detected pollutant column mapping:', pollutant_column_map)\n",
    "\n",
    "    # Now iterate chunks, compute pollutant AQIs and overall AQI\n",
    "    chunksize = 200_000\n",
    "    first = True\n",
    "    total_rows = 0\n",
    "    aqi_computed_count = 0\n",
    "    # we'll write the augmented CSV with added columns: _aqi_<pollutant> and aqi, aqi_main_pollutant\n",
    "    for chunk in pd.read_csv(std_path, chunksize=chunksize):\n",
    "        total_rows += len(chunk)\n",
    "        # compute AQI per pollutant\n",
    "        pollutant_aqis = {}\n",
    "        for pol, colname in pollutant_column_map.items():\n",
    "            if colname in chunk.columns:\n",
    "                pollutant_aqis[pol] = chunk[colname].map(lambda v: aqi_calc_from_breakpoints(v, breakpoints[pol]))\n",
    "                # name the column\n",
    "                chunk[f'aqi_{pol}'] = pollutant_aqis[pol]\n",
    "        # compute overall AQI and primary pollutant\n",
    "        if pollutant_aqis:\n",
    "            # DataFrame of pollutant AQIs\n",
    "            aqi_df = pd.DataFrame({pol: pollutant_aqis[pol] for pol in pollutant_aqis})\n",
    "            # overall AQI is row-wise max (ignoring NaN)\n",
    "            chunk['aqi_computed'] = aqi_df.max(axis=1)\n",
    "            # primary pollutant: pollutant with max aqi per row\n",
    "            def top_pol(row):\n",
    "                row = row.to_dict()\n",
    "                # choose pollutant with max AQI\n",
    "                best = None\n",
    "                best_val = -1\n",
    "                for k, v in row.items():\n",
    "                    if pd.isna(v):\n",
    "                        continue\n",
    "                    if v > best_val:\n",
    "                        best_val = v\n",
    "                        best = k\n",
    "                return best\n",
    "            chunk['aqi_main_pollutant'] = aqi_df.apply(top_pol, axis=1)\n",
    "            # if existing 'aqi' column present, prefer computed values where available\n",
    "            if 'aqi' in chunk.columns:\n",
    "                # count how many computed will replace or fill\n",
    "                will_fill = int(chunk['aqi_computed'].notna().sum())\n",
    "                aqi_computed_count += will_fill\n",
    "                # fill existing aqi where missing, otherwise keep existing\n",
    "                chunk['aqi'] = chunk['aqi'].fillna(chunk['aqi_computed'])\n",
    "            else:\n",
    "                chunk['aqi'] = chunk['aqi_computed']\n",
    "                aqi_computed_count += int(chunk['aqi'].notna().sum())\n",
    "            # drop the helper column 'aqi_computed'\n",
    "            chunk.drop(columns=['aqi_computed'], inplace=True)\n",
    "        else:\n",
    "            print('No pollutant columns detected for AQI computation in this dataset; skipping per-chunk AQI calc')\n",
    "\n",
    "        # write augmented chunk\n",
    "        chunk.to_csv(out_aqi, mode='a', index=False, header=first)\n",
    "        first = False\n",
    "\n",
    "    print('\\nWrote AQI-augmented CSV to:', out_aqi)\n",
    "    print('Total rows processed:', total_rows)\n",
    "    print('AQI values computed/fill count:', aqi_computed_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58054b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source PDF not found: C:\\Projects\\us-air-quality-dashboard\\jupyter_notebooks\\jupyter_notebooks\\outputs\\cleaned_datasets_summary.pdf\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Source PDF (relative to project root) and destination path (absolute)\n",
    "output_pdf = Path(\"jupyter_notebooks/outputs/cleaned_datasets_summary.pdf\")\n",
    "export_copy = Path(\"C:/Users/ifrah/Documents/cleaned_datasets_summary.pdf\")\n",
    "\n",
    "if not output_pdf.exists():\n",
    "    print(f\"Source PDF not found: {output_pdf.resolve()}\")\n",
    "else:\n",
    "    try:\n",
    "        # Ensure destination directory exists\n",
    "        export_copy.parent.mkdir(parents=True, exist_ok=True)\n",
    "        # use copy2 to preserve metadata where possible\n",
    "        shutil.copy2(str(output_pdf), str(export_copy))\n",
    "        print(\"Exported a copy to:\", export_copy.resolve())\n",
    "    except Exception as e:\n",
    "        print(\"Failed to copy PDF:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
