{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42578cff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:50:22.048918Z",
     "iopub.status.busy": "2025-11-13T09:50:22.033731Z",
     "iopub.status.idle": "2025-11-13T09:50:22.070077Z",
     "shell.execute_reply": "2025-11-13T09:50:22.070077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: c:\\Projects\\us-air-quality-dashboard\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "cwd = Path.cwd()\n",
    "if cwd.name == 'jupyter_notebooks':\n",
    "    os.chdir(cwd.parent)\n",
    "print('Working dir:', Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b52be15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote cleaned CSV to outputs\\pollution_us_200_2016_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# Chunk-processing (only runs if raw exists)\n",
    "from pathlib import Path\n",
    "file_path = Path('Data/pollution_us_2000_2016.csv')\n",
    "out_path = Path('outputs/pollution_us_200_2016_clean.csv')\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "if file_path.exists():\n",
    "    if out_path.exists():\n",
    "        out_path.unlink()\n",
    "    chunks = pd.read_csv(file_path, chunksize=200_000)\n",
    "    first = True\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk['Date Local'] = pd.to_datetime(chunk.get('Date Local'), errors='coerce')\n",
    "        chunk['Year'] = chunk['Date Local'].dt.year\n",
    "        chunk['Month'] = chunk['Date Local'].dt.month\n",
    "        chunk['Quarter'] = chunk['Date Local'].dt.quarter\n",
    "        chunk.to_csv(out_path, mode='a', index=False, header=first)\n",
    "        first = False\n",
    "    print('Wrote cleaned CSV to', out_path)\n",
    "else:\n",
    "    print('Raw file missing; chunk-processing skipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc047fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardization input: outputs\\pollution_us_200_2016_clean.csv\n",
      "Detected numeric columns: ['unnamed_0', 'state_code', 'county_code', 'site_num', 'no2_mean', 'no2_1st_max_value', 'no2_1st_max_hour', 'no2_aqi', 'o3_mean', 'o3_1st_max_value', 'o3_1st_max_hour', 'o3_aqi', 'so2_mean', 'so2_1st_max_value', 'so2_1st_max_hour', 'so2_aqi', 'co_mean', 'co_1st_max_value', 'co_1st_max_hour', 'co_aqi', 'year', 'month', 'quarter']\n",
      "Computed medians for numeric columns (sample): {'unnamed_0': 999.5, 'state_code': 4.0, 'county_code': 13.0, 'site_num': 3002.0, 'no2_mean': 29.6742425, 'no2_1st_max_value': 55.0, 'no2_1st_max_hour': 19.0, 'no2_aqi': 52.0}\n",
      "Finished chunked processing — wrote to outputs\\pollution_us_200_2016_clean_numeric_standardized.csv\n",
      "Total rows processed (approx): 1746661\n",
      "Standardized file sample rows: 5\n",
      "Columns: ['unnamed_0', 'state_code', 'county_code', 'site_num', 'address', 'state', 'county', 'city', 'date_local', 'no2_units', 'no2_mean', 'no2_1st_max_value', 'no2_1st_max_hour', 'no2_aqi', 'o3_units', 'o3_mean', 'o3_1st_max_value', 'o3_1st_max_hour', 'o3_aqi', 'so2_units']\n",
      "Finished chunked processing — wrote to outputs\\pollution_us_200_2016_clean_numeric_standardized.csv\n",
      "Total rows processed (approx): 1746661\n",
      "Standardized file sample rows: 5\n",
      "Columns: ['unnamed_0', 'state_code', 'county_code', 'site_num', 'address', 'state', 'county', 'city', 'date_local', 'no2_units', 'no2_mean', 'no2_1st_max_value', 'no2_1st_max_hour', 'no2_aqi', 'o3_units', 'o3_mean', 'o3_1st_max_value', 'o3_1st_max_hour', 'o3_aqi', 'so2_units']\n"
     ]
    }
   ],
   "source": [
    "# Step 4 : Standardization\n",
    "# This cell is inserted from scripts/step4_standardize.py\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def to_snake(s):\n",
    "    if s is None:\n",
    "        return s\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r'[\\/\\\\]+', '_', s)\n",
    "    s = re.sub(r'\\s+', '_', s)\n",
    "    s = re.sub(r'[^0-9a-z_]', '', s)\n",
    "    s = re.sub(r'_+', '_', s)\n",
    "    return s\n",
    "\n",
    "# Choose input file\n",
    "in_path = Path('outputs/pollution_us_200_2016_clean_numeric_fixed.csv')\n",
    "fallbacks = [Path('outputs/pollution_us_200_2016_clean.csv'), Path('outputs/pollution_us_2000_2016_clean.csv')]\n",
    "if not in_path.exists():\n",
    "    in_path = next((p for p in fallbacks if p.exists()), None)\n",
    "\n",
    "if in_path is None:\n",
    "    print('No input file found for standardization; abort Step 4')\n",
    "else:\n",
    "    print('Standardization input:', in_path)\n",
    "    # sample to infer column names and numeric candidates\n",
    "    sample = pd.read_csv(in_path, nrows=2000)\n",
    "    col_map = {c: to_snake(c) for c in sample.columns}\n",
    "    sample = sample.rename(columns=col_map)\n",
    "\n",
    "    # detect numeric columns (dtype-based and coercion heuristic)\n",
    "    numeric_cols = [c for c in sample.columns if pd.api.types.is_numeric_dtype(sample[c])]\n",
    "    if not numeric_cols:\n",
    "        numeric_cols = []\n",
    "        for c in sample.columns:\n",
    "            non_null = sample[c].notna().sum()\n",
    "            if non_null == 0:\n",
    "                continue\n",
    "            coerced = pd.to_numeric(sample[c].astype(str).str.replace(',', '.'), errors='coerce')\n",
    "            if coerced.notna().sum() / float(non_null) >= 0.5:\n",
    "                numeric_cols.append(c)\n",
    "    print('Detected numeric columns:', numeric_cols)\n",
    "\n",
    "    medians = {}\n",
    "    if numeric_cols:\n",
    "        medians = sample[numeric_cols].median().to_dict()\n",
    "    print('Computed medians for numeric columns (sample):', {k: medians.get(k) for k in list(medians)[:8]})\n",
    "\n",
    "    if 'aqi' in sample.columns and sample['aqi'].notna().any():\n",
    "        overall_aqi_med = sample['aqi'].dropna().median()\n",
    "        aqi_by_year = sample.groupby('year')['aqi'].median().to_dict() if 'year' in sample.columns else {}\n",
    "    else:\n",
    "        overall_aqi_med = None\n",
    "        aqi_by_year = {}\n",
    "\n",
    "    out_path = Path('outputs/pollution_us_200_2016_clean_numeric_standardized.csv')\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_path.exists():\n",
    "        out_path.unlink()\n",
    "\n",
    "    chunksize = 200_000\n",
    "    total_rows = 0\n",
    "    aqi_before = 0\n",
    "    aqi_after = 0\n",
    "    first = True\n",
    "\n",
    "    for chunk in pd.read_csv(in_path, chunksize=chunksize):\n",
    "        chunk.rename(columns=col_map, inplace=True)\n",
    "        total_rows += len(chunk)\n",
    "\n",
    "        obj_cols = chunk.select_dtypes(include=['object']).columns\n",
    "        for col in obj_cols:\n",
    "            if chunk[col].astype(str).str.contains(',', na=False).any():\n",
    "                chunk[col] = chunk[col].astype(str).str.replace(',', '.', regex=False)\n",
    "\n",
    "        for c in numeric_cols:\n",
    "            if c in chunk.columns:\n",
    "                chunk[c] = pd.to_numeric(chunk[c].astype(str).str.replace(',', '.'), errors='coerce')\n",
    "                if c in medians and pd.notna(medians.get(c)):\n",
    "                    chunk[c] = chunk[c].fillna(medians[c])\n",
    "\n",
    "        if 'aqi' in chunk.columns:\n",
    "            aqi_before += int(chunk['aqi'].isna().sum())\n",
    "            if aqi_by_year:\n",
    "                def _fill(row):\n",
    "                    if pd.notna(row.get('aqi')):\n",
    "                        return row.get('aqi')\n",
    "                    y = row.get('year') if 'year' in row.index else None\n",
    "                    if pd.notna(y) and y in aqi_by_year and pd.notna(aqi_by_year[y]):\n",
    "                        return aqi_by_year[y]\n",
    "                    return overall_aqi_med\n",
    "                chunk['aqi'] = chunk.apply(_fill, axis=1)\n",
    "            else:\n",
    "                if overall_aqi_med is not None:\n",
    "                    chunk['aqi'] = chunk['aqi'].fillna(overall_aqi_med)\n",
    "            aqi_after += int(chunk['aqi'].isna().sum())\n",
    "\n",
    "        chunk.to_csv(out_path, mode='a', index=False, header=first)\n",
    "        first = False\n",
    "\n",
    "    print('Finished chunked processing — wrote to', out_path)\n",
    "    print('Total rows processed (approx):', total_rows)\n",
    "    if overall_aqi_med is not None:\n",
    "        print('AQI missing before:', aqi_before, 'after:', aqi_after)\n",
    "    try:\n",
    "        sample_out = pd.read_csv(out_path, nrows=5)\n",
    "        print('Standardized file sample rows:', len(sample_out))\n",
    "        print('Columns:', sample_out.columns.tolist()[:20])\n",
    "    except Exception as e:\n",
    "        print('Could not read output sample:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18207496",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:50:22.070077Z",
     "iopub.status.busy": "2025-11-13T09:50:22.070077Z",
     "iopub.status.idle": "2025-11-13T09:50:24.909544Z",
     "shell.execute_reply": "2025-11-13T09:50:24.909544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 rows from Data\\pollution_us_2000_2016.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>State Code</th>\n",
       "      <th>County Code</th>\n",
       "      <th>Site Num</th>\n",
       "      <th>Address</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>City</th>\n",
       "      <th>Date Local</th>\n",
       "      <th>NO2 Units</th>\n",
       "      <th>...</th>\n",
       "      <th>SO2 Units</th>\n",
       "      <th>SO2 Mean</th>\n",
       "      <th>SO2 1st Max Value</th>\n",
       "      <th>SO2 1st Max Hour</th>\n",
       "      <th>SO2 AQI</th>\n",
       "      <th>CO Units</th>\n",
       "      <th>CO Mean</th>\n",
       "      <th>CO 1st Max Value</th>\n",
       "      <th>CO 1st Max Hour</th>\n",
       "      <th>CO AQI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>1.145833</td>\n",
       "      <td>4.2</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>2.2</td>\n",
       "      <td>23</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>2.975000</td>\n",
       "      <td>6.6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>1.145833</td>\n",
       "      <td>4.2</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>2.975000</td>\n",
       "      <td>6.6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>2.2</td>\n",
       "      <td>23</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3002</td>\n",
       "      <td>1645 E ROOSEVELT ST-CENTRAL PHOENIX STN</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Maricopa</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>...</td>\n",
       "      <td>Parts per billion</td>\n",
       "      <td>1.958333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Parts per million</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>1.6</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  State Code  County Code  Site Num  \\\n",
       "0           0           4           13      3002   \n",
       "1           1           4           13      3002   \n",
       "2           2           4           13      3002   \n",
       "3           3           4           13      3002   \n",
       "4           4           4           13      3002   \n",
       "\n",
       "                                   Address    State    County     City  \\\n",
       "0  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN  Arizona  Maricopa  Phoenix   \n",
       "1  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN  Arizona  Maricopa  Phoenix   \n",
       "2  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN  Arizona  Maricopa  Phoenix   \n",
       "3  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN  Arizona  Maricopa  Phoenix   \n",
       "4  1645 E ROOSEVELT ST-CENTRAL PHOENIX STN  Arizona  Maricopa  Phoenix   \n",
       "\n",
       "   Date Local          NO2 Units  ...          SO2 Units  SO2 Mean  \\\n",
       "0  2000-01-01  Parts per billion  ...  Parts per billion  3.000000   \n",
       "1  2000-01-01  Parts per billion  ...  Parts per billion  3.000000   \n",
       "2  2000-01-01  Parts per billion  ...  Parts per billion  2.975000   \n",
       "3  2000-01-01  Parts per billion  ...  Parts per billion  2.975000   \n",
       "4  2000-01-02  Parts per billion  ...  Parts per billion  1.958333   \n",
       "\n",
       "   SO2 1st Max Value  SO2 1st Max Hour SO2 AQI           CO Units   CO Mean  \\\n",
       "0                9.0                21    13.0  Parts per million  1.145833   \n",
       "1                9.0                21    13.0  Parts per million  0.878947   \n",
       "2                6.6                23     NaN  Parts per million  1.145833   \n",
       "3                6.6                23     NaN  Parts per million  0.878947   \n",
       "4                3.0                22     4.0  Parts per million  0.850000   \n",
       "\n",
       "   CO 1st Max Value  CO 1st Max Hour CO AQI  \n",
       "0               4.2               21    NaN  \n",
       "1               2.2               23   25.0  \n",
       "2               4.2               21    NaN  \n",
       "3               2.2               23   25.0  \n",
       "4               1.6               23    NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "src = Path('Data/pollution_us_2000_2016.csv')\n",
    "fallback = Path('outputs/pollution_us_200_2016_clean.csv')\n",
    "if src.exists():\n",
    "    df = pd.read_csv(src, nrows=10000)\n",
    "    print(f'Loaded {len(df)} rows from {src}')\n",
    "elif fallback.exists():\n",
    "    df = pd.read_csv(fallback, nrows=10000)\n",
    "    print(f'Fallback: loaded {len(df)} rows from {fallback}')\n",
    "else:\n",
    "    print('No data available; creating empty DataFrame')\n",
    "    df = pd.DataFrame()\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "841a45a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected pollutant column mapping: {'so2': 'so2_units', 'no2': 'no2_units', 'co': 'state_code', 'o3': 'o3_units'}\n",
      "\n",
      "Wrote AQI-augmented CSV to: outputs\\pollution_us_200_2016_with_aqi.csv\n",
      "Total rows processed: 1746661\n",
      "AQI values computed/fill count: 1746661\n",
      "\n",
      "Wrote AQI-augmented CSV to: outputs\\pollution_us_200_2016_with_aqi.csv\n",
      "Total rows processed: 1746661\n",
      "AQI values computed/fill count: 1746661\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Calculate AQI per pollutant and compute overall AQI; write augmented CSV\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Input is the standardized CSV from Step 4 if present\n",
    "std_path = Path('outputs/pollution_us_200_2016_clean_numeric_standardized.csv')\n",
    "if not std_path.exists():\n",
    "    print('Standardized input not found; skipping AQI calculation (expected at', std_path, ')')\n",
    "else:\n",
    "    out_aqi = Path('outputs/pollution_us_200_2016_with_aqi.csv')\n",
    "    out_aqi.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_aqi.exists():\n",
    "        print('Removing existing AQI file:', out_aqi)\n",
    "        out_aqi.unlink()\n",
    "\n",
    "    # EPA-like breakpoints for AQI calculation (assumes concentrations in common units)\n",
    "    # Each entry is list of tuples: (C_low, C_high, I_low, I_high)\n",
    "    breakpoints = {\n",
    "        'pm25': [\n",
    "            (0.0, 12.0, 0, 50),\n",
    "            (12.1, 35.4, 51, 100),\n",
    "            (35.5, 55.4, 101, 150),\n",
    "            (55.5, 150.4, 151, 200),\n",
    "            (150.5, 250.4, 201, 300),\n",
    "            (250.5, 350.4, 301, 400),\n",
    "            (350.5, 500.4, 401, 500)\n",
    "        ],\n",
    "        'pm10': [\n",
    "            (0, 54, 0, 50),\n",
    "            (55, 154, 51, 100),\n",
    "            (155, 254, 101, 150),\n",
    "            (255, 354, 151, 200),\n",
    "            (355, 424, 201, 300),\n",
    "            (425, 504, 301, 400),\n",
    "            (505, 604, 401, 500)\n",
    "        ],\n",
    "        # SO2, NO2 are in ppb here; CO in ppm; O3 in ppb (8-hr)\n",
    "        'so2': [\n",
    "            (0, 35, 0, 50),\n",
    "            (36, 75, 51, 100),\n",
    "            (76, 185, 101, 150),\n",
    "            (186, 304, 151, 200),\n",
    "            (305, 604, 201, 300),\n",
    "            (605, 804, 301, 400),\n",
    "            (805, 1004, 401, 500)\n",
    "        ],\n",
    "        'no2': [\n",
    "            (0, 53, 0, 50),\n",
    "            (54, 100, 51, 100),\n",
    "            (101, 360, 101, 150),\n",
    "            (361, 649, 151, 200),\n",
    "            (650, 1249, 201, 300),\n",
    "            (1250, 1649, 301, 400),\n",
    "            (1650, 2049, 401, 500)\n",
    "        ],\n",
    "        'co': [\n",
    "            (0.0, 4.4, 0, 50),\n",
    "            (4.5, 9.4, 51, 100),\n",
    "            (9.5, 12.4, 101, 150),\n",
    "            (12.5, 15.4, 151, 200),\n",
    "            (15.5, 30.4, 201, 300),\n",
    "            (30.5, 40.4, 301, 400),\n",
    "            (40.5, 50.4, 401, 500)\n",
    "        ],\n",
    "        'o3': [\n",
    "            (0, 54, 0, 50),\n",
    "            (55, 70, 51, 100),\n",
    "            (71, 85, 101, 150),\n",
    "            (86, 105, 151, 200),\n",
    "            (106, 200, 201, 300)\n",
    "            # higher ranges omitted for brevity\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    def aqi_calc_from_breakpoints(C, bps):\n",
    "        \"\"\"Calculate AQI for a concentration C given breakpoint list bps\"\"\"\n",
    "        if C is None or (isinstance(C, float) and math.isnan(C)):\n",
    "            return None\n",
    "        try:\n",
    "            C = float(C)\n",
    "        except Exception:\n",
    "            return None\n",
    "        for (Cl, Ch, Il, Ih) in bps:\n",
    "            if Cl <= C <= Ch:\n",
    "                # linear interpolation\n",
    "                aqi = (Ih - Il) / (Ch - Cl) * (C - Cl) + Il\n",
    "                return int(round(aqi))\n",
    "        # if outside defined range, clip to 500 or return None\n",
    "        if C > bps[-1][1]:\n",
    "            return 500\n",
    "        return None\n",
    "\n",
    "    # find candidate column names in the standardized file that map to pollutants\n",
    "    sample = pd.read_csv(std_path, nrows=5)\n",
    "    cols = sample.columns.tolist()\n",
    "    pollutant_column_map = {}\n",
    "    # heuristics for column name detection\n",
    "    mapping_hints = {\n",
    "        'pm25': ['pm2_5', 'pm25', 'pm_2_5', 'pm2.5'],\n",
    "        'pm10': ['pm10', 'pm_10'],\n",
    "        'so2': ['so2', 's02', 'sulfur_dioxide'],\n",
    "        'no2': ['no2', 'nitrogen_dioxide'],\n",
    "        'co': ['co', 'carbon_monoxide'],\n",
    "        'o3': ['o3', 'ozone']\n",
    "    }\n",
    "    for pol, hints in mapping_hints.items():\n",
    "        for h in hints:\n",
    "            matches = [c for c in cols if h in c]\n",
    "            if matches:\n",
    "                pollutant_column_map[pol] = matches[0]\n",
    "                break\n",
    "\n",
    "    print('Detected pollutant column mapping:', pollutant_column_map)\n",
    "\n",
    "    # Now iterate chunks, compute pollutant AQIs and overall AQI\n",
    "    chunksize = 200_000\n",
    "    first = True\n",
    "    total_rows = 0\n",
    "    aqi_computed_count = 0\n",
    "    # we'll write the augmented CSV with added columns: _aqi_<pollutant> and aqi, aqi_main_pollutant\n",
    "    for chunk in pd.read_csv(std_path, chunksize=chunksize):\n",
    "        total_rows += len(chunk)\n",
    "        # compute AQI per pollutant\n",
    "        pollutant_aqis = {}\n",
    "        for pol, colname in pollutant_column_map.items():\n",
    "            if colname in chunk.columns:\n",
    "                pollutant_aqis[pol] = chunk[colname].map(lambda v: aqi_calc_from_breakpoints(v, breakpoints[pol]))\n",
    "                # name the column\n",
    "                chunk[f'aqi_{pol}'] = pollutant_aqis[pol]\n",
    "        # compute overall AQI and primary pollutant\n",
    "        if pollutant_aqis:\n",
    "            # DataFrame of pollutant AQIs\n",
    "            aqi_df = pd.DataFrame({pol: pollutant_aqis[pol] for pol in pollutant_aqis})\n",
    "            # overall AQI is row-wise max (ignoring NaN)\n",
    "            chunk['aqi_computed'] = aqi_df.max(axis=1)\n",
    "            # primary pollutant: pollutant with max aqi per row\n",
    "            def top_pol(row):\n",
    "                row = row.to_dict()\n",
    "                # choose pollutant with max AQI\n",
    "                best = None\n",
    "                best_val = -1\n",
    "                for k, v in row.items():\n",
    "                    if pd.isna(v):\n",
    "                        continue\n",
    "                    if v > best_val:\n",
    "                        best_val = v\n",
    "                        best = k\n",
    "                return best\n",
    "            chunk['aqi_main_pollutant'] = aqi_df.apply(top_pol, axis=1)\n",
    "            # if existing 'aqi' column present, prefer computed values where available\n",
    "            if 'aqi' in chunk.columns:\n",
    "                # count how many computed will replace or fill\n",
    "                will_fill = int(chunk['aqi_computed'].notna().sum())\n",
    "                aqi_computed_count += will_fill\n",
    "                # fill existing aqi where missing, otherwise keep existing\n",
    "                chunk['aqi'] = chunk['aqi'].fillna(chunk['aqi_computed'])\n",
    "            else:\n",
    "                chunk['aqi'] = chunk['aqi_computed']\n",
    "                aqi_computed_count += int(chunk['aqi'].notna().sum())\n",
    "            # drop the helper column 'aqi_computed'\n",
    "            chunk.drop(columns=['aqi_computed'], inplace=True)\n",
    "        else:\n",
    "            print('No pollutant columns detected for AQI computation in this dataset; skipping per-chunk AQI calc')\n",
    "\n",
    "        # write augmented chunk\n",
    "        chunk.to_csv(out_aqi, mode='a', index=False, header=first)\n",
    "        first = False\n",
    "\n",
    "    print('\\nWrote AQI-augmented CSV to:', out_aqi)\n",
    "    print('Total rows processed:', total_rows)\n",
    "    print('AQI values computed/fill count:', aqi_computed_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58054b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source PDF not found: C:\\Projects\\us-air-quality-dashboard\\jupyter_notebooks\\outputs\\cleaned_datasets_summary.pdf\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Source PDF (relative to project root) and destination path (absolute)\n",
    "output_pdf = Path(\"jupyter_notebooks/outputs/cleaned_datasets_summary.pdf\")\n",
    "export_copy = Path(\"C:/Users/ifrah/Documents/cleaned_datasets_summary.pdf\")\n",
    "\n",
    "if not output_pdf.exists():\n",
    "    print(f\"Source PDF not found: {output_pdf.resolve()}\")\n",
    "else:\n",
    "    try:\n",
    "        # Ensure destination directory exists\n",
    "        export_copy.parent.mkdir(parents=True, exist_ok=True)\n",
    "        # use copy2 to preserve metadata where possible\n",
    "        shutil.copy2(str(output_pdf), str(export_copy))\n",
    "        print(\"Exported a copy to:\", export_copy.resolve())\n",
    "    except Exception as e:\n",
    "        print(\"Failed to copy PDF:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
